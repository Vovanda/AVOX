
# Proof of Hypothesis: Personalized Adaptive Learning Model (PALM)

автор: **Владимир Савкин (Vladimir Savkin)**
---

> В ходе разработки своей RAG-системы меня посетила следующая мысль: "А как мне "скормить" системе мою любимую книгу? Или пять книг?! Смогу ли я получить точную, непротиворечивую информацию без дополнительных запросов? Может ли RAG знать всё о моих данных?" Я уверен, что для RAG-системы это непосильная задача.
Особенно остро это проявляется в юриспруденции: человек загружает в систему огромный договор, затем идут дополнения и уточнения, но сам договор не обновляется. Такая ситуация показалась фатальной для RAG, и я спросил себя: **какая может быть альтернатива?**

## Введение

В современных LLM-системах проблема обновления и персонализации знаний остаётся нерешённой. **RAG** (Retrieval-Augmented Generation) позволяет временно подмешивать новые данные не изменяя саму модель. **Fine-tuning** всей модели LLM даёт ..., но требует значительных вычислительных ресурсов.

Я предлагаю **стратегию PALM (Personalized Adaptive Learning Model)**, которая сочетает адаптеры и RAG, обеспечивая баланс между гибкостью и устойчивостью знаний.
Основные механизмы PALM:
- **Гибридное селективное внимание (Hybrid Selective Attention)**: важные данные интегрируются в адаптер, менее критичные временно остаются в RAG.
- **Интеграция с RAG**: при запросе новые данные сначала подаются на вход LLM,работая как классический RAG, а затем, при накоплении **критической массы**, попадают в fine-tuning.
- "Актуализация данных в нутри LLM при достижении "критической массы" данных**: обучение происходит не постоянно, а когда накапливается достаточное количество данных.

Эта стратегия **развивает и дополняет** адаптерные методы, а не заменяет их.

## Архитектура модели

Обозначим общую структуру модели:

- **[|||** — энкодер: извлекает скрытые представления входного текста.
- **]|[** — адаптер(ы) или персонализированные слои: отвечают за персонализацию знаний и подстраиваются под новый контекст. Эти слои малы по сравнению со всей моделью и легко взаимозаменяемы, что позволяет адаптировать модель под различные типы данных и пользователей.
- **|||]** — декодер: формирует ответы на основе полученной информации, включая правила и навыки LLM.

> Только **]|[** (слои адаптера) дообучается во время fine-tuning, а остальная часть модели остаётся неизменной. При этом компоненты модели естественным образом соединяются в "Персонализированную LLM" **[|||]|[|||]**.


## Подходы к созданию тренировочной выборки

### Идея и постановка задачи

Для решения поставленной задачи я предлагаю использовать стратегию **PALM (Personalized Adaptive Learning Model)**, которая сочетает адаптеры и RAG, обеспечивая баланс между гибкостью и устойчивостью знаний. 
При тюнинге адаптеров важно формировать тренировочную выборку так, чтобы модель **усваивала новые данные, но не теряла критически важные знания** ниже 
  
Однако не все данные необходимо сразу включать в обучение — часть информации можно подавать через RAG, пока она не наберёт **критическую массу**.

Основные принципы PALM:

- **Гибридное селективное внимание (Hybrid Selective Attention)**:
  - Важные и подтверждённые знания интегрируются в адаптер в процессе обучения.
  - Менее критичная или временная информация остаётся в RAG до её окончательной валидации.

- **Алгоритм ранжирования данных для обучения адаптеров** 

- Старые документы или их фрагменты остаются в датасете, если они сохраняют свою актуальность.

- Важные документы всегда включаются в обучающую выборку, независимо от времени их появления в системе.

- Менее значимые, но свежие данные участвуют в обучении частично.

- Новые документы изначально обрабатываются через RAG, а затем могут быть добавлены в адаптер, если их значимость подтверждена или накоплен достаточный объем данных.

- RAG работает на малом корпусе данных преимущественно состоящим из новой, еще не "усвоенной" системой, пользовательской информации: сообщения пользователя, различные документы и т.д..


Подоьная стратегия позволяет эффективно интегрировать новые данные в модель, не нарушая уже накопленные знания, и обеспечивает стабильность и актуальность информации.

#### Ранжирование важности информации

Ранжирование важности приходящих документов \( I(D_i) \) основывается на анализе количества документов, схожих по содержанию и смыслу, в хранилище.
Это можно реализовать, используя LSH для фрагментов документов или переводя документы в эмбеддинги и выполняя поиск схожих элементов в векторной базе данных или любой другой вариант подходящий для вас.

#### Формальная модель выборки

Обозначим множество документов как \( D \). Каждому документу \( D_i \) сопоставим:
- \( t(D_i) \) — момент времени поступления документа.
- \( I(D_i) \) — мера важности документа (оценка на основе количества релевантных документов в векторной БД).
- \( S(D_i) \) — значимость конкретных частей документа (анализируется на основе выделения ключевой информации).

#### Тренировочная выборка \( T \) формируется следующим образом:

##### Документы, подлежащие fine-tuning:
- **Важные документы независимо от времени:**
  \[
  D_t^{important} = \{ D_i \in D \mid I(D_i) > I_{threshold} \}
  \]

- **Ключевые фрагменты старых документов:**
  \[
  D_t^{selected} = \{ S(D_i) \mid D_i \in D, t(D_i) < t_{threshold}, I(D_i) > I_{threshold} \}
  \]

- **Менее важные, но относительно новые документы:**
  \[
  D_t^{moderate} = \{ D_i \in D \mid t(D_i) > t_{recent}, I(D_i) < I_{threshold} \}
  \]

##### Документы, подаваемые через RAG (временно, без fine-tuning):
- **Все новые документы:**
  \[
  D_t^{new} = \{ D_i \in D \mid t(D_i) > t_{latest} \}
  \]

#### Финальная выборка для тюнинга адаптера:

\[
T = D_t^{important} \cup D_t^{selected} \cup D_t^{moderate}
\]

Дополнительно:
- Пока документ находится в \( D_t^{new} \), он используется только через RAG.
- Когда набирается достаточная масса новых данных, они проходят валидацию (не обязательно) и могут быть добавлены в процес тюнинга персонализированых слоев.

---

## Почему это работает?

Этот алгоритм учитывает ключевые слабые места классического RAG:

1. **Разрыв в согласованности информации**:
   - В RAG новые данные не доступны все сразу, но и вывод из-за недостатка контекста может противоречить актуальной информации. При этом при различных не сильно четких запросах, итог системы модет быть также различным.
   - В предложенном подходе свежие данные сначала идут в RAG, а потом — в fine-tuning, если подтверждены. RAG нацелен на небольшой объем данных, что 

2. **Устаревание информации**:
   - В RAG достаточно сложно удалить или исправить устаревшие данные, что связано с необходимостью значительных ресурсов для отслеживания и актуализации информации при большом объеме данных. Задача не является тривиальной с точки зрения алгоритмов, и ресурсы часто тратятся без значительного улучшения точности ответов.
   - В стратегии PALM адаптер система ориентирована на сохранение наиболее важных данных, понимая их в контексте целого. Данные, которые система усвоила, но которые со временем становятся менее востребованными, могут быть постепенно забыты. Для объективности стоит отметить, что мы также тратим ресурсы на постоянный тюнинг адаптера, что влечёт за собой определённые издержки, однако это позволяет сохранять актуальность и целосность знаний, повышая качество ответов.

3. **Ограниченность контекстного окна**:
   - В классическом RAG приходится загружать большие объемы данных, которые не всегда подходят для текущего контекста, что ведет к избыточному использованию токенов.
   - В PALM важные знания сразу переходят в , освобождая контекст. В этой модели RAG работает с гораздо меньшими объемами данных, поскольку лишь временно обрабатывает информацию, которая ещё не прошла через валидацию и адаптацию. Это позволяет эффективно использовать контекстное окно, минимизируя расход токенов.


---

### Альтернативные подходы

1. **Sliding Window Sampling** — учитывает только последние данные, но игнорирует старые знания.
2. **Bayesian Importance Estimation** — динамически обновляет веса важности документов, что сложнее в реализации.
---

