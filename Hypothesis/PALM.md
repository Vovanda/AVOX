
# Proof of Hypothesis: Personalized Adaptive Learning Model (PALM)

автор: **Владимир Савкин (Vladimir Savkin)**
---

> Я в ходе разработки своей RAG-системы меня посетила мысль: "А как мне "скормить" системе, например, мою любимую книгу? Или пять? Смогу ли я получить точную, непротиворечивую информацию без дополнительных запросов?" Особенно остро это проявляется в юриспруденции: человек загружает в систему огромный договор, затем идут дополнения и уточнения, но сам договор не обновляется. Такая ситуация показалась фатальной для RAG, и я спросил себя: **какая может быть альтернатива?**

## Введение

В современных LLM-системах проблема обновления и персонализации знаний остаётся нерешённой. **RAG** (Retrieval-Augmented Generation) позволяет временно подмешивать новые данные, но не изменяет саму модель. **Fine-tuning** даёт долговременные обновления, но требует значительных вычислительных ресурсов.

Я предлагаю **стратегию PALM (Personalized Adaptive Learning Model)**, которая сочетает адаптеры и RAG, обеспечивая баланс между гибкостью и устойчивостью знаний. Основные принципы PALM:
- **Гибридное селективное внимание (Hybrid Selective Attention)**: важные данные интегрируются в адаптер, менее критичные временно остаются в RAG.
- **Гибкая интеграция RAG**: новые данные сначала подаются через RAG, а затем, при накоплении **критической массы**, попадают в fine-tuning.
- **Обновление адаптера по "критической массе" данных**: обучение происходит не постоянно, а когда накапливается достаточное количество релевантных данных.

Эта стратегия **развивает и дополняет** адаптерные методы, а не заменяет их.

## Архитектура модели

Обозначим общую структуру модели:

- **[|||** — энкодер: извлекает скрытые представления входного текста.
- **]|[** — адаптер(ы) или персонализированные слои: отвечают за персонализацию знаний и подстраиваются под новый контекст. Эти слои малы по сравнению со всей моделью и легко взаимозаменяемы, что позволяет адаптировать модель под различные типы данных и пользователей.
- **|||]** — декодер: формирует ответы на основе полученной информации, включая правила и навыки LLM.

> Только **]|[** (блок с адаптером) дообучается во время fine-tuning, а остальная часть модели остаётся неизменной. При этом компоненты модели естественным образом соединяются в "Персонализированную LLM" **[|||]|[|||]**.


## Подходы к созданию тренировочной выборки

### Идея и постановка задачи

Я предлагаю **стратегию PALM (Personalized Adaptive Learning Model)**, которая сочетает адаптеры и RAG, обеспечивая баланс между гибкостью и устойчивостью знаний. При тюнинге адаптеров важно формировать тренировочную выборку так, чтобы модель **усваивала новые данные, но не теряла критически важные знания**.
  
Однако не все данные необходимо сразу включать в обучение — часть информации можно подавать через RAG, пока она не наберёт **критическую массу**.

Основные принципы PALM:

- **Гибридное селективное внимание (Hybrid Selective Attention)**:
  - Важные и подтверждённые знания интегрируются в адаптер через fine-tuning.
  - Менее критичная или временная информация остаётся в RAG до её окончательной валидации.

- **Алгоритм ранжирования данных для обучения адаптера**:
  - Старые документы или их фрагменты сохраняются в датасете, если они всё ещё содержат важную информацию.
  - Важные документы всегда включаются в обучение, независимо от времени.
  - Менее важные, но новые документы частично участвуют в обучении.
  - Новые документы сначала проходят через RAG, а затем могут быть включены в fine-tuning, если их значимость подтверждается.

Этот подход позволяет эффективно интегрировать новые данные в модель, не нарушая уже накопленные знания, и обеспечивает стабильность и актуальность информации.

#### Ранжирование важности информации

Ранжирование важности документов \( I(D_i) \) происходит на основе анализа количества релевантных документов в векторной БД.


#### Формальная модель выборки

Обозначим множество документов как \( D \). Каждому документу \( D_i \) сопоставим:
- \( t(D_i) \) — момент времени поступления документа.
- \( I(D_i) \) — мера важности документа (оценка на основе количества релевантных документов в векторной БД).
- \( S(D_i) \) — значимость конкретных частей документа (анализируется на основе выделения ключевой информации).

#### Тренировочная выборка \( T \) формируется следующим образом:

##### Документы, подлежащие fine-tuning:
- **Важные документы независимо от времени:**
  \[
  D_t^{important} = \{ D_i \in D \mid I(D_i) > I_{threshold} \}
  \]

- **Ключевые фрагменты старых документов:**
  \[
  D_t^{selected} = \{ S(D_i) \mid D_i \in D, t(D_i) < t_{threshold}, I(D_i) > I_{threshold} \}
  \]

- **Менее важные, но относительно новые документы:**
  \[
  D_t^{moderate} = \{ D_i \in D \mid t(D_i) > t_{recent}, I(D_i) < I_{threshold} \}
  \]

##### Документы, подаваемые через RAG (временно, без fine-tuning):
- **Все новые документы:**
  \[
  D_t^{new} = \{ D_i \in D \mid t(D_i) > t_{latest} \}
  \]

#### Финальная выборка для тюнинга адаптера:

\[
T = D_t^{important} \cup D_t^{selected} \cup D_t^{moderate}
\]

Дополнительно:
- Пока документ находится в \( D_t^{new} \), он используется только через RAG.
- Когда набирается достаточная масса новых данных, они проходят валидацию и могут быть добавлены в fine-tuning.

---

## Почему это работает?

Этот алгоритм учитывает ключевые слабые места классического RAG:

1. **Разрыв в согласованности информации**:
   - В RAG новые данные не доступны все сразу, но и вывод из-за недостатка контекста может противоречить актуальной информации. При этом при различных не сильно четких запросах, итог системы модет быть также различным.
   - В предложенном подходе свежие данные сначала идут в RAG, а потом — в fine-tuning, если подтверждены. RAG нацелен на небольшой объем данных, что 

2. **Устаревание информации**:
   - В RAG достаточно сложно удалить или исправить устаревшие данные, что связано с необходимостью значительных ресурсов для отслеживания и актуализации информации при большом объеме данных. Задача не является тривиальной с точки зрения алгоритмов, и ресурсы часто тратятся без значительного улучшения точности ответов.
   - В стратегии PALM адаптер обновляется, сохраняя только релевантные знания. Однако стоит отметить, что мы также тратим ресурсы на постоянный тюнинг адаптера, что влечёт за собой определённые издержки, однако это позволяет сохранять актуальность и точность знаний.

3. **Ограниченность контекстного окна**:
   - В классическом RAG приходится загружать большие объемы данных, которые не всегда подходят для текущего контекста, что ведет к избыточному использованию токенов.
   - В PALM важные знания сразу переходят в fine-tuning, освобождая контекст. В этой модели RAG работает с гораздо меньшими объемами данных, поскольку лишь временно обрабатывает информацию, которая ещё не прошла через валидацию и адаптацию. Это позволяет эффективно использовать контекстное окно, минимизируя расход токенов.
---

### Альтернативные подходы

1. **Sliding Window Sampling** — учитывает только последние данные, но игнорирует старые знания.
2. **Bayesian Importance Estimation** — динамически обновляет веса важности документов, что сложнее в реализации.
3. **Чистый Hybrid Selective Attention** — аналогичный предложенному подходу, но без тюнинга адаптера (всё остаётся в RAG).

Этот метод позволяет гибко комбинировать RAG и fine-tuning, обеспечивая актуальность и согласованность информации.

---

