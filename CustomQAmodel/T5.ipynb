{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from py.misс import JSONDataLoader, extract_sbsq\n",
    "from transformers import AutoTokenizer, T5Tokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from py.models.model import QAModule\n",
    "from py.models.data import SquadQADataset\n",
    "\n",
    "# Параметры модели и данных\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from py.models.model import QAModule\n",
    "\n",
    "MODEL_PATH = \".//checkpoints//best_model-epoch=00-val_loss=0.36.ckpt\"\n",
    "\n",
    "tokenizer: T5Tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH, legacy=False)\n",
    "# Загрузка обученной модели из checkpoint\n",
    "model = QAModule.load_from_checkpoint(MODEL_PATH)\n",
    "\n",
    "# Перемещение на GPU/CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Для использования модели:\n",
    "def use_model(question, context):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        f\"[КОНТЕКСТ] {context} [ВОПРОС] {question}\",\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.model.generate(\n",
    "            inputs['input_ids'].to(device),\n",
    "            attention_mask=inputs['attention_mask'].to(device)\n",
    "        )\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\SawKing\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--google_bleu\\6fc70b7be0088120a372dfdd5d320b39b8bb3630cb8029b193941d9376e86bb0 (last modified on Sun Jul 21 18:44:14 2024) since it couldn't be found locally at evaluate-metric--google_bleu, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\SawKing\\Documents\\T5\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metric\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Оценка модели на валидационном наборе\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results)\n",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, val_loader, device)\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      4\u001b[0m metric \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle_bleu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m      8\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\SawKing\\Documents\\T5\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\SawKing\\Documents\\T5\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\SawKing\\Documents\\T5\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\SawKing\\Documents\\T5\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\SawKing\\Documents\\T5\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Функция для валидации\n",
    "def validate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    metric = evaluate.load(\"google_bleu\")\n",
    "    \n",
    "\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Извлечение меток\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Замена -100 на pad_token_id\n",
    "        labels = torch.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Генерация предсказаний\n",
    "            outputs = model.model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=32)\n",
    "\n",
    "        # Декодирование предсказаний и меток\n",
    "        preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        targets = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Обновление метрики\n",
    "        for pred, target in zip(preds, targets):\n",
    "            metric.add(prediction=pred, reference=target)\n",
    "\n",
    "    return metric.compute()\n",
    "\n",
    "# Оценка модели на валидационном наборе\n",
    "results = validate_model(trained_model, val_loader, device)\n",
    "print(\"Validation Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def generate_answer(\n",
    "    model: QAModule,\n",
    "    tokenizer,\n",
    "    question,\n",
    "    ref_answer=None,\n",
    "    device= None,\n",
    "    text_max_token_len: int = 396\n",
    "):\n",
    "    \"\"\"\n",
    "    Генерация ответа на вопрос с использованием модели.\n",
    "\n",
    "    Args:\n",
    "        model (QAModule): Модель для генерации ответов.\n",
    "        tokenizer: Токенизатор для преобразования текста.\n",
    "        question (dict): Словарь с ключами 'question' и 'context'.\n",
    "        ref_answer (str, optional): Референсный ответ для вычисления метрики. Defaults to None.\n",
    "        device: Устройство для выполнения (CPU/GPU). Defaults to CPU.\n",
    "        text_max_token_len (int, optional): Максимальная длина текста. Defaults to 396.\n",
    "\n",
    "    Returns:\n",
    "        dict: Содержит предсказанный ответ и, если передан `ref_answer`, BLEU метрику.\n",
    "    \"\"\"\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)    \n",
    "    \n",
    "    # Токенизация вопроса и контекста\n",
    "    inputs = tokenizer(\n",
    "        f\"[КОНТЕКСТ]{question['context']}[ВОПРОС]|{question['question']}\",\n",
    "        max_length=text_max_token_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    #print(f\"Device: {device}\")\n",
    "    #print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    #print(f\"Inputs device: {inputs['input_ids'].device}\")    \n",
    "    #print(f\"attention_mask: {inputs[\"attention_mask\"].device}\")\n",
    "    \n",
    "    # Генерация ответа\n",
    "    generated_ids = model.model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        num_beams=2,\n",
    "        max_length=50,\n",
    "        repetition_penalty=2.5,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True,\n",
    "        use_cache=False,\n",
    "    )\n",
    "    \n",
    "    predicted_answer = tokenizer.decode(\n",
    "        generated_ids.flatten(),\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "    )\n",
    "    \n",
    "    # Вычисление BLEU метрики, если есть референсный ответ\n",
    "    result = {\"Предсказанный ответ\": predicted_answer}\n",
    "    if ref_answer:\n",
    "        try:\n",
    "            bleu = evaluate.load(\"google_bleu\")\n",
    "            score = bleu.compute(predictions=[predicted_answer], references=[ref_answer])\n",
    "            result[\"Референсный ответ\"] = ref_answer\n",
    "            result[\"BLEU Score\"] = score[\"google_bleu\"]\n",
    "        except Exception as e:\n",
    "            result[\"BLEU Score\"] = f\"Error computing BLEU: {e}\"\n",
    "    \n",
    "    # Вывод дополнительной информации\n",
    "    if ref_answer:\n",
    "        print(\"Контекст:\\n\", question[\"context\"])\n",
    "        print(\"\\nВопрос:\\n\", question[\"question\"])\n",
    "        print(\"\\nПредсказанный ответ:\\n\", predicted_answer)\n",
    "        print(\"\\nРеференсный ответ:\\n\", ref_answer)\n",
    "        if \"BLEU Score\" in result:\n",
    "            print(\"\\nBLEU Score:\\n\", result[\"BLEU Score\"])\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Контекст:\n",
      " Банк был основан предпринимателем Олегом Тиньковым в 2006 году под названием Тинькофф Кредитные Системы . По словам Тинькова, он заинтересовался моделью дистанционного обслуживания американского банка Wells Fargo и монолайнера Capital One, специализирующегося на банковских картах. Вместе с консультантами из Boston Consulting Group он пришёл к выводу, что модель дистанционного кредитного банка может работать в России. Для получения лицензии на банковскую деятельность он приобрёл Химмашбанк , небольшой кэптивный банк, занимавшийся обслуживанием предприятий из химической и фармацевтической отрасли. Предприниматель вложил в открытие банка без отделений 70 млн долларов США из своего восьмидесятимиллионного состояния.\n",
      "\n",
      "Вопрос:\n",
      " Вместе с консультантами из какой компании Тиньков пришёл к выводу, что модель дистанционного кредитного банка может работать в России?\n",
      "\n",
      "Предсказанный ответ:\n",
      " Восточный\n",
      "\n",
      "Референсный ответ:\n",
      " из Boston Consulting Group\n",
      "\n",
      "BLEU Score:\n",
      " 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Предсказанный ответ': 'Восточный',\n",
       " 'Референсный ответ': 'из Boston Consulting Group',\n",
       " 'BLEU Score': 0.0}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_sample = val_df.sample().iloc[0]\n",
    "generate_answer(trained_model, tokenizer, question = qa_sample, ref_answer = qa_sample[\"answer_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Контекст:\n",
      " Русский бандит настиг американского афериста в офисе.\n",
      "\n",
      "Вопрос:\n",
      " В чём сила?\n",
      "\n",
      "Предсказанный ответ:\n",
      " В чём сила\n",
      "\n",
      "Референсный ответ:\n",
      " В правде\n",
      "\n",
      "BLEU Score:\n",
      " 0.16666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Предсказанный ответ': 'В чём сила',\n",
       " 'Референсный ответ': 'В правде',\n",
       " 'BLEU Score': 0.16666666666666666}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_question = {\n",
    "    \"context\": \"Русский бандит настиг американского афериста в офисе.\",\n",
    "    \"question\": \"В чём сила?\",\n",
    "    \"answer_text\": \"В правде\"\n",
    "}\n",
    "\n",
    "generate_answer(trained_model, tokenizer, question= my_question, ref_answer = my_question[\"answer_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
